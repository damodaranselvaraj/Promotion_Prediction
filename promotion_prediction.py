# -*- coding: utf-8 -*-
"""Promotion_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15HL6qJwWboYDYM1tV7N-6MC0IMyfpNiN
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import warnings

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

from sklearn.compose import ColumnTransformer

from sklearn.linear_model import LogisticRegression

from sklearn.ensemble import RandomForestClassifier as RandomForest

from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report

from sklearn.model_selection import GridSearchCV

from imblearn.over_sampling import RandomOverSampler

warnings.filterwarnings('ignore')

# Read the input dataset
df = pd.read_csv("/content/sample_data/train_Promotion.csv")

# Check numbebr of Row and Column in the input file
df.shape

# Get the datatypes for each column
df.info()

# Check the first few records in the input file
df.head(7)

# Get all the Column names.
df.columns

#Remove the Employee ID as it is a unique values. It doesnot have any impace in the prediction instead it is a noise.
df = df.drop(columns=['employee_id'], errors='ignore')
df.columns

# Check whether any rows of data are duplicted.
df.duplicated().sum()

# Remove duplicate rows from the dataframe
df =df.drop_duplicates()
df.shape

#check the count of null values in each column
df.isna().sum()

#Remove null value with mode.
df["previous_year_rating"] = df["previous_year_rating"].fillna(df["previous_year_rating"].mode()[0])
df.isna().sum()

#Remove null value with Mode as it is catagorical
df["education"] = df["education"].fillna(df["education"].mode()[0])
df.isna().sum()

df.dtypes

df["previous_year_rating"].unique()

df["previous_year_rating"] = df["previous_year_rating"].astype(int)
df.dtypes

df.nunique()

df['department'].unique()

df['education'].unique()

df['region'].unique()

df.describe()

sns.boxplot(x=df['education'])

df['length_of_service'].unique()

df['length_of_service'].hist()

df['education'].value_counts()

df['department'].value_counts()

df['is_promoted'].value_counts()

# Select only numeric columns for correlation calculation
numeric_df = df.select_dtypes(exclude='object')
sns.heatmap(numeric_df.corr(), annot=True)

#Spliting the Columns into X and y. Independent columns into X and dependent column into y.
# is_promoted is the target value.
X = df.drop(columns=['is_promoted'], axis=1)
y = df['is_promoted']

# This splits the overall data into train and test in the ratio of 80/20.
train_X,test_X,train_y,test_y = train_test_split(X,y,test_size=0.2,random_state=42)

# Check the shape of the each cateory
train_X.shape,test_X.shape,train_y.shape,test_y.shape

#Split Category and Numeric column separately
cat_cols = X.select_dtypes(include='object').columns
num_cols = X.select_dtypes(exclude='object').columns

cat_cols

num_cols

# Create a pipeline for Imputer to replace null value and encode the values.
cat_pipe_endc = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),
                                ('encoder',OneHotEncoder(handle_unknown='ignore'))])
cat_pipe_endc

# Create a pipeline for Numeric values to replace null value by median and standarize all the column
num_pipe_endc = Pipeline(steps=[('imputer',SimpleImputer(strategy='median')),
                                ('scaler',StandardScaler())])
num_pipe_endc

# Create a Transformer for both Category and Numeric column
preprocess = ColumnTransformer(transformers=[
    ('cat_enc', cat_pipe_endc, cat_cols),
    ('num_enc', num_pipe_endc, num_cols)
    ])

# Integrate preprocessing step and Model in a pipeline
lr_model_pipeline = Pipeline(steps=[
    ('preprocess', preprocess),
    ('model', LogisticRegression())
])
lr_model_pipeline

# Fit the model. Actual training a=happens in this step.
lr_model_pipeline.fit(train_X, train_y)

lr_predict = lr_model_pipeline.predict(test_X)

lr_model_pipeline.predict_proba(test_X)

# Repeat the same for Random Forest
rf_model_pipeline = Pipeline(steps=[
    ('preprocess', preprocess),
    ('model', RandomForest())
])
rf_model_pipeline

# Fit the model
rf_model_pipeline.fit(train_X, train_y)

rf_predict = rf_model_pipeline.predict(test_X)

rf_model_pipeline.predict_proba(test_X)

# Check the F1 score for both of the models
lr_f1 = f1_score(test_y, lr_predict)
rf_f1 = f1_score(test_y, rf_predict)
print("Logistic Regression F1 Score: ", lr_f1)
print("Random Forest F1 Score: ", rf_f1)

# Create oversampler file
over_sampling = RandomOverSampler(random_state=42)

train_X_os, train_y_os = over_sampling.fit_resample(train_X, train_y)

# Check the shape of the dataframe after the Oversampling.
train_X_os.shape, train_y_os.shape

# Set parameters and Create a GridSearchCV pipeline
lr_param_grid = {
    'model__C': [0.01, 0.1, 1, 10],
    'model__penalty': ['l2'],
    'model__max_iter': [500, 700]
}
lr_grid = GridSearchCV(
    estimator=lr_model_pipeline,
    param_grid=lr_param_grid,
    cv=5,
    scoring='f1'
)

# Train using GridSearchCV
lr_grid.fit(train_X_os, train_y_os)
print("Best Params:", lr_grid.best_params_)
print("Best CV Score:", lr_grid.best_score_)

best_lr_model = lr_grid.best_estimator_

lr_pred = best_lr_model.predict(test_X)
lr_prob = best_lr_model.predict_proba(test_X)
lr_f1 = f1_score(test_y, lr_pred)
print("F1 Score:", lr_f1)

rf_param_grid = {
    'model__n_estimators': [100, 200],
    'model__max_depth': [5, 10],
    'model__min_samples_split': [2, 5],
    'model__min_samples_leaf': [1, 2]
}

rf_grid = GridSearchCV(
    estimator=rf_model_pipeline,
    param_grid=rf_param_grid,
    cv=5,
    scoring='f1'
)

rf_grid.fit(train_X_os, train_y_os)
print("Best Params:", rf_grid.best_params_)
print("Best CV Score:", rf_grid.best_score_)

best_rf_model = rf_grid.best_estimator_

rf_pred = best_rf_model.predict(test_X)
rf_prob = best_rf_model.predict_proba(test_X)
rf_f1 = f1_score(test_y, rf_pred)
print("F1 Score:", rf_f1)

# Preparation for submission
test_df = pd.read_csv("/content/test_Promotion.csv")
test_predictions = best_lr_model.predict(test_df)

submission = pd.DataFrame({
    'employee_id': test_df['employee_id'],
    'is_promoted': test_predictions
})
submission.to_csv("submission.csv", index=False)